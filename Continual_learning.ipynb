{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa337d3e",
   "metadata": {},
   "source": [
    "## Setup environment\n",
    "\n",
    "Goal: Learn the optimal threshold for a classification problem, objective function used here is the F1 score.\n",
    "\n",
    "1. There is a real optimal threshold that is passed as a parameter to the environment, this will be unknown to the agent, but is used to generate feedback such as TP, FP, FN and F1 score (used as the objective here).\n",
    "2. The agent starts at threshold = 0.5 and takes continuous actions between (-0.5, 0.5). The agent state traverses the space (0,1).\n",
    "3. The agent samples 'draws_per_objective_calculation' number of samples from this feedback data and computes the F1 score\n",
    "4. The agent performs a linear decay exploration strategy to perform single step episodes (superflous RL).\n",
    "5. Reward is characterized as the objective function here.\n",
    "6. Steps 4 and 5 turn this into an optimization problem.\n",
    "7. Run several episodes\n",
    "8. The state, action pair with the highest reward is noted (This is very different from an RL problem)\n",
    "9. The target optimal threshold is the state calculated using the state action pair (s,a) such that Q(s,a) is the maximum value (1.0 here). This gives target threshold state = s + a\n",
    "10. This can run continually and identify new thresholds given enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "752c8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from gym.wrappers import TimeLimit\n",
    "from gym.spaces import Discrete, Box\n",
    "from d3rlpy.algos import DQN\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy, ConstantEpsilonGreedy\n",
    "\n",
    "\n",
    "class Continuous_states(gym.Env):\n",
    "    \"\"\"Example of a custom env in which you have to walk down a corridor.\n",
    "    Get a reward of -0.1 if you are not at the end, a random reward that is positive if you do.\n",
    "    Move +1 if you move forward, -1 if you move backward. The total length is 5.\n",
    "    We should want to reach the end in 5 steps in the perfectly trained world. \n",
    "    You can configure the length of the corridor via the env config.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.end_pos = 1\n",
    "        self.start_pos = 0.5\n",
    "        self.cur_pos = np.array([self.start_pos])\n",
    "        self.episode_samples = 0\n",
    "        self.max_episode_samples = 1\n",
    "        \n",
    "        self.total_time = 0\n",
    "        self.draws_per_objective_calculation = 100\n",
    "        self.data_dist = np.random.uniform\n",
    "        self.real_threshold = config['real_threshold']\n",
    "        \n",
    "        self.action_space = Box(-0.5, 0.5, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = Box(0, 1, shape=(1,), dtype=np.float32)\n",
    "        # Set the seed. This is only used for the final (reach goal) reward.\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        random.seed(seed)\n",
    "        pos = random.uniform(a=self.start_pos - 0.5 , b=self.start_pos + 0.5)\n",
    "        self.cur_pos = np.array([pos])\n",
    "        self.episode_samples = 0\n",
    "        return self.cur_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        assert ((action <= 0.5) and  (action >= -0.5))\n",
    "        \n",
    "        objective = self.objective()\n",
    "        \n",
    "        #if(objective > 0.9):\n",
    "        #    reward_scale = 0.5\n",
    "        #elif((objective >= 0.75) and (objective < 0.9)):\n",
    "        #    reward_scale = 0.2\n",
    "        #elif((objective >= 0.5) and (objective < 75)):\n",
    "        #    reward_scale = 0.01\n",
    "        #elif(objective < 0.5):\n",
    "        #    reward_scale = -0.1 * (0.5 - objective)\n",
    "            \n",
    "        reward_scale = objective\n",
    "        \n",
    "        print(f\"Current state: {self.cur_pos}, action: {action}, objective: {objective}, reward: {reward_scale}\")\n",
    "        #print(self.cur_pos[0], (self.cur_pos[0] > 0.99) or (self.cur_pos[0] < 0.001))\n",
    "            \n",
    "        reward = reward_scale\n",
    "        \n",
    "        self.cur_pos = self.cur_pos + action\n",
    "        \n",
    "        self.episode_samples += 1\n",
    "        \n",
    "        done = truncated = False\n",
    "        \n",
    "        if(self.episode_samples == self.max_episode_samples) or (action > 0.5) or (action < -0.5) or (self.cur_pos[0] > 0.99) or (self.cur_pos[0] < 0.001):\n",
    "            done = truncated = True\n",
    "            print(done)\n",
    "            \n",
    "        # Produce a random reward when we reach the goal.\n",
    "        return (\n",
    "            self.cur_pos,\n",
    "            reward, # Setting to 2 instead of random reward has no real impact\n",
    "            truncated,\n",
    "            {},\n",
    "        )\n",
    "    \n",
    "    def objective(self):\n",
    "        draws = self.data_dist(size=self.draws_per_objective_calculation)\n",
    "        classification = draws > self.cur_pos\n",
    "        true_label = draws > self.real_threshold\n",
    "        tp = classification & true_label\n",
    "        fp = classification & ~(true_label)\n",
    "        fn = ~(classification) & true_label\n",
    "        f1 = tp.sum() / (tp.sum() + 0.5 *(fp.sum() + fn.sum()))\n",
    "        objective = f1\n",
    "        return(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abafe4",
   "metadata": {},
   "source": [
    "### Run CQL to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "e1950494",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-17 17:56:37 [warning  ] Unused arguments are passed.   learning_rate=2.5e-07 target_update_interval=100\n",
      "2023-03-17 17:56:37 [info     ] Directory is created at d3rlpy_logs/SAC_online_20230317175637\n",
      "2023-03-17 17:56:37 [debug    ] Fitting scaler...              scler=min_max\n",
      "2023-03-17 17:56:37 [debug    ] Building model...\n",
      "2023-03-17 17:56:37 [debug    ] Model has been built.\n",
      "2023-03-17 17:56:37 [info     ] Parameters are saved to d3rlpy_logs/SAC_online_20230317175637/params.json params={'action_scaler': None, 'actor_encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'actor_learning_rate': 0.0003, 'actor_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'batch_size': 32, 'critic_encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'critic_learning_rate': 0.0003, 'critic_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'gamma': 0.99, 'generated_maxlen': 100000, 'initial_temperature': 1.0, 'n_critics': 2, 'n_frames': 1, 'n_steps': 1, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': {'type': 'min_max', 'params': {'maximum': array([[1.]], dtype=float32), 'minimum': array([[0.]], dtype=float32)}}, 'tau': 0.005, 'temp_learning_rate': 0.0003, 'temp_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'use_gpu': None, 'algorithm': 'SAC', 'observation_shape': (1,), 'action_size': 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d7e7d1d1b742af8d99b4e0adf62a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: [0.73571322], action: [0.], objective: 0.6666666666666666, reward: 0.6666666666666666\n",
      "True\n",
      "Current state: [0.68837905], action: [0.], objective: 0.6976744186046512, reward: 0.6976744186046512\n",
      "True\n",
      "Current state: [0.45711476], action: [0.], objective: 1.0, reward: 1.0\n",
      "True\n",
      "Current state: [0.46656059], action: [0.], objective: 0.9904761904761905, reward: 0.9904761904761905\n",
      "True\n",
      "Current state: [0.96853198], action: [0.], objective: 0.11764705882352941, reward: 0.11764705882352941\n",
      "True\n",
      "Current state: [0.90804456], action: [0.], objective: 0.2153846153846154, reward: 0.2153846153846154\n",
      "True\n",
      "Current state: [0.23567273], action: [0.], objective: 0.8031496062992126, reward: 0.8031496062992126\n",
      "True\n",
      "Current state: [0.96269357], action: [0.], objective: 0.2222222222222222, reward: 0.2222222222222222\n",
      "True\n",
      "Current state: [0.31779343], action: [0.], objective: 0.8947368421052632, reward: 0.8947368421052632\n",
      "True\n",
      "Current state: [0.653259], action: [0.], objective: 0.7575757575757576, reward: 0.7575757575757576\n",
      "True\n",
      "Current state: [1.42606325e-05], action: [0.], objective: 0.7730061349693251, reward: 0.7730061349693251\n",
      "True\n",
      "Current state: [0.52942274], action: [0.], objective: 0.9545454545454546, reward: 0.9545454545454546\n",
      "True\n",
      "Current state: [0.95378321], action: [0.], objective: 0.12307692307692308, reward: 0.12307692307692308\n",
      "True\n",
      "Current state: [0.34988758], action: [0.], objective: 0.9016393442622951, reward: 0.9016393442622951\n",
      "True\n",
      "Current state: [0.87691485], action: [-0.18508525], objective: 0.43478260869565216, reward: 0.43478260869565216\n",
      "True\n",
      "Current state: [0.69096446], action: [0.], objective: 0.6868686868686869, reward: 0.6868686868686869\n",
      "True\n",
      "Current state: [0.28140954], action: [0.], objective: 0.9076923076923077, reward: 0.9076923076923077\n",
      "True\n",
      "Current state: [0.97298573], action: [-0.19796583], objective: 0.06153846153846154, reward: 0.06153846153846154\n",
      "True\n",
      "Current state: [0.81278234], action: [0.], objective: 0.5679012345679012, reward: 0.5679012345679012\n",
      "True\n",
      "Current state: [0.42825067], action: [0.], objective: 0.9904761904761905, reward: 0.9904761904761905\n",
      "True\n",
      "Current state: [0.88322003], action: [0.], objective: 0.3142857142857143, reward: 0.3142857142857143\n",
      "True\n",
      "Current state: [0.90519181], action: [0.], objective: 0.32727272727272727, reward: 0.32727272727272727\n",
      "True\n",
      "Current state: [0.95339694], action: [0.], objective: 0.1935483870967742, reward: 0.1935483870967742\n",
      "True\n",
      "Current state: [0.1970328], action: [0.], objective: 0.8309859154929577, reward: 0.8309859154929577\n",
      "True\n",
      "Current state: [0.26910666], action: [0.], objective: 0.8571428571428571, reward: 0.8571428571428571\n",
      "True\n",
      "Current state: [0.88567296], action: [-0.1864114], objective: 0.375, reward: 0.375\n",
      "True\n",
      "Current state: [0.75254295], action: [-0.1671432], objective: 0.6, reward: 0.6\n",
      "True\n",
      "Current state: [0.22083598], action: [-0.10899253], objective: 0.8091603053435115, reward: 0.8091603053435115\n",
      "True\n",
      "Current state: [0.26918206], action: [-0.11317269], objective: 0.8217054263565892, reward: 0.8217054263565892\n",
      "True\n",
      "Current state: [0.15238485], action: [-0.10444637], objective: 0.7972972972972973, reward: 0.7972972972972973\n",
      "True\n",
      "Current state: [0.05904102], action: [-0.10458849], objective: 0.7567567567567568, reward: 0.7567567567567568\n",
      "True\n",
      "Current state: [0.01169545], action: [-0.10387727], objective: 0.6666666666666666, reward: 0.6666666666666666\n",
      "True\n",
      "Current state: [0.73124181], action: [-0.16427948], objective: 0.6265060240963856, reward: 0.6265060240963856\n",
      "True\n",
      "Current state: [0.11826011], action: [-0.10358826], objective: 0.8157894736842105, reward: 0.8157894736842105\n",
      "True\n",
      "Current state: [0.02561625], action: [-0.10433263], objective: 0.7320261437908496, reward: 0.7320261437908496\n",
      "True\n",
      "2023-03-17 17:56:37 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230317175637/model_25.pt\n",
      "2023-03-17 17:56:37 [info     ] SAC_online_20230317175637: epoch=1 step=25 epoch=1 metrics={'time_inference': 0.0013936614990234374, 'time_environment_step': 0.0003811454772949219, 'rollout_return': 0.6116118530456935, 'time_step': 0.0019157695770263671, 'evaluation': 0.7000908093992267} step=25\n",
      "Current state: [-0.07871637], action: [0.], objective: 0.7261146496815286, reward: 0.7261146496815286\n",
      "True\n",
      "Current state: [0.48660103], action: [0.], objective: 0.9914529914529915, reward: 0.9914529914529915\n",
      "True\n",
      "Current state: [0.60322013], action: [0.], objective: 0.8604651162790697, reward: 0.8604651162790697\n",
      "True\n",
      "Current state: [0.11325625], action: [0.], objective: 0.7794117647058824, reward: 0.7794117647058824\n",
      "True\n",
      "Current state: [0.07149094], action: [0.], objective: 0.7516778523489933, reward: 0.7516778523489933\n",
      "True\n",
      "Current state: [0.24445641], action: [0.], objective: 0.8455284552845529, reward: 0.8455284552845529\n",
      "True\n",
      "Current state: [0.80747218], action: [-0.17553826], objective: 0.5526315789473685, reward: 0.5526315789473685\n",
      "True\n",
      "Current state: [0.86474962], action: [0.], objective: 0.34782608695652173, reward: 0.34782608695652173\n",
      "True\n",
      "Current state: [0.68586503], action: [-0.1574178], objective: 0.7380952380952381, reward: 0.7380952380952381\n",
      "True\n",
      "Current state: [0.28998042], action: [0.], objective: 0.8805970149253731, reward: 0.8805970149253731\n",
      "True\n",
      "Current state: [0.49816375], action: [0.], objective: 0.9433962264150944, reward: 0.9433962264150944\n",
      "True\n",
      "Current state: [0.55654539], action: [0.], objective: 0.9090909090909091, reward: 0.9090909090909091\n",
      "True\n",
      "Current state: [0.18289], action: [-0.10581259], objective: 0.8226950354609929, reward: 0.8226950354609929\n",
      "True\n",
      "Current state: [0.8781534], action: [0.], objective: 0.26229508196721313, reward: 0.26229508196721313\n",
      "True\n",
      "Current state: [0.75647922], action: [0.], objective: 0.631578947368421, reward: 0.631578947368421\n",
      "True\n",
      "Current state: [0.4785633], action: [0.], objective: 0.9914529914529915, reward: 0.9914529914529915\n",
      "True\n",
      "Current state: [0.47727461], action: [0.], objective: 0.9803921568627451, reward: 0.9803921568627451\n",
      "True\n",
      "Current state: [0.03573997], action: [0.], objective: 0.759493670886076, reward: 0.759493670886076\n",
      "True\n",
      "Current state: [0.63562373], action: [0.], objective: 0.8235294117647058, reward: 0.8235294117647058\n",
      "True\n",
      "Current state: [0.29703075], action: [0.], objective: 0.8188976377952756, reward: 0.8188976377952756\n",
      "True\n",
      "Current state: [0.67597776], action: [0.], objective: 0.6966292134831461, reward: 0.6966292134831461\n",
      "True\n",
      "Current state: [0.9478361], action: [0.], objective: 0.25, reward: 0.25\n",
      "True\n",
      "Current state: [0.52108706], action: [0.], objective: 0.8932038834951457, reward: 0.8932038834951457\n",
      "True\n",
      "Current state: [0.9653108], action: [0.], objective: 0.0784313725490196, reward: 0.0784313725490196\n",
      "True\n",
      "Current state: [0.30349972], action: [-0.09220871], objective: 0.8372093023255814, reward: 0.8372093023255814\n",
      "True\n",
      "Current state: [0.90140234], action: [-0.13677609], objective: 0.26865671641791045, reward: 0.26865671641791045\n",
      "True\n",
      "Current state: [0.07872747], action: [-0.06018939], objective: 0.7014925373134329, reward: 0.7014925373134329\n",
      "True\n",
      "Current state: [0.81539831], action: [-0.12766016], objective: 0.4, reward: 0.4\n",
      "True\n",
      "Current state: [0.20569636], action: [-0.06542204], objective: 0.8115942028985508, reward: 0.8115942028985508\n",
      "True\n",
      "Current state: [0.40103946], action: [-0.08436553], objective: 0.967741935483871, reward: 0.967741935483871\n",
      "True\n",
      "Current state: [0.94403984], action: [-0.14120342], objective: 0.25, reward: 0.25\n",
      "True\n",
      "Current state: [0.34362409], action: [-0.07887191], objective: 0.890625, reward: 0.890625\n",
      "True\n",
      "Current state: [0.30753366], action: [-0.07468711], objective: 0.896, reward: 0.896\n",
      "True\n",
      "Current state: [0.19032611], action: [-0.06433059], objective: 0.8378378378378378, reward: 0.8378378378378378\n",
      "True\n",
      "Current state: [0.90856282], action: [-0.1375304], objective: 0.20689655172413793, reward: 0.20689655172413793\n",
      "True\n",
      "2023-03-17 17:56:37 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230317175637/model_50.pt\n",
      "2023-03-17 17:56:37 [info     ] SAC_online_20230317175637: epoch=2 step=50 epoch=2 metrics={'time_inference': 0.0009911155700683594, 'time_environment_step': 0.000301055908203125, 'rollout_return': 0.7268838635837936, 'time_step': 0.0024871730804443357, 'time_sample_batch': 0.00010311603546142578, 'time_algorithm_update': 0.013429045677185059, 'temp_loss': 1.599435269832611, 'temp': 0.999549925327301, 'critic_loss': 0.9854676127433777, 'actor_loss': -0.6509155631065369, 'evaluation': 0.6230844781675741} step=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: [0.77103242], action: [0.], objective: 0.6933333333333334, reward: 0.6933333333333334\n",
      "Current state: [0.77103242], action: [0.], objective: 0.5526315789473685, reward: 0.5526315789473685\n",
      "Current state: [0.77103242], action: [0.], objective: 0.6285714285714286, reward: 0.6285714285714286\n",
      "Current state: [0.77103242], action: [-0.12214544], objective: 0.5789473684210527, reward: 0.5789473684210527\n",
      "Current state: [0.64888697], action: [0.], objective: 0.7160493827160493, reward: 0.7160493827160493\n",
      "Current state: [0.64888697], action: [0.], objective: 0.8444444444444444, reward: 0.8444444444444444\n",
      "Current state: [0.64888697], action: [-0.10833202], objective: 0.8041237113402062, reward: 0.8041237113402062\n",
      "Current state: [0.54055495], action: [0.], objective: 0.9072164948453608, reward: 0.9072164948453608\n",
      "Current state: [0.54055495], action: [-0.09843021], objective: 0.9666666666666667, reward: 0.9666666666666667\n",
      "Current state: [0.44212474], action: [-0.08868174], objective: 1.0, reward: 1.0\n",
      "Current state: [0.35344301], action: [-0.05502847], objective: 0.9508196721311475, reward: 0.9508196721311475\n",
      "Current state: [0.29841454], action: [0.], objective: 0.9264705882352942, reward: 0.9264705882352942\n",
      "Current state: [0.29841454], action: [0.], objective: 0.8823529411764706, reward: 0.8823529411764706\n",
      "Current state: [0.29841454], action: [0.], objective: 0.8444444444444444, reward: 0.8444444444444444\n",
      "Current state: [0.29841454], action: [0.], objective: 0.8709677419354839, reward: 0.8709677419354839\n",
      "Current state: [0.29841454], action: [0.], objective: 0.9007633587786259, reward: 0.9007633587786259\n",
      "Current state: [0.29841454], action: [0.], objective: 0.8833333333333333, reward: 0.8833333333333333\n",
      "Current state: [0.29841454], action: [0.], objective: 0.8592592592592593, reward: 0.8592592592592593\n",
      "Current state: [0.29841454], action: [0.], objective: 0.8688524590163934, reward: 0.8688524590163934\n",
      "Current state: [0.29841454], action: [0.], objective: 0.835820895522388, reward: 0.835820895522388\n",
      "Current state: [0.29841454], action: [-0.03243594], objective: 0.8939393939393939, reward: 0.8939393939393939\n",
      "Current state: [0.2659786], action: [0.], objective: 0.8936170212765957, reward: 0.8936170212765957\n",
      "Current state: [0.2659786], action: [0.], objective: 0.8740740740740741, reward: 0.8740740740740741\n",
      "Current state: [0.2659786], action: [-0.02910129], objective: 0.896, reward: 0.896\n",
      "Current state: [0.23687731], action: [0.], objective: 0.8571428571428571, reward: 0.8571428571428571\n",
      "Current state: [0.78315534], action: [-0.07759812], objective: 0.4931506849315068, reward: 0.4931506849315068\n",
      "True\n",
      "Current state: [0.32617685], action: [-0.03580127], objective: 0.8448275862068966, reward: 0.8448275862068966\n",
      "True\n",
      "Current state: [0.14702008], action: [-0.02031264], objective: 0.7352941176470589, reward: 0.7352941176470589\n",
      "True\n",
      "Current state: [0.67646148], action: [-0.06732096], objective: 0.7567567567567568, reward: 0.7567567567567568\n",
      "True\n",
      "Current state: [0.69575138], action: [-0.06908235], objective: 0.631578947368421, reward: 0.631578947368421\n",
      "True\n",
      "Current state: [0.50380108], action: [-0.05368242], objective: 0.9444444444444444, reward: 0.9444444444444444\n",
      "True\n",
      "Current state: [0.02409119], action: [-0.01395366], objective: 0.7564102564102564, reward: 0.7564102564102564\n",
      "True\n",
      "Current state: [0.64987968], action: [-0.06493263], objective: 0.6666666666666666, reward: 0.6666666666666666\n",
      "True\n",
      "Current state: [0.91937137], action: [-0.08966248], objective: 0.23333333333333334, reward: 0.23333333333333334\n",
      "True\n",
      "Current state: [0.06905307], action: [-0.0157658], objective: 0.7333333333333333, reward: 0.7333333333333333\n",
      "True\n",
      "2023-03-17 17:56:38 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230317175637/model_75.pt\n",
      "2023-03-17 17:56:38 [info     ] SAC_online_20230317175637: epoch=3 step=75 epoch=3 metrics={'time_inference': 0.001155529022216797, 'time_environment_step': 0.0003246784210205078, 'time_step': 0.002657003402709961, 'time_sample_batch': 0.00012683868408203125, 'time_algorithm_update': 0.01388239860534668, 'temp_loss': 1.7022876143455505, 'temp': 0.9989489018917084, 'critic_loss': 1.5091155767440796, 'actor_loss': -0.8704869747161865, 'evaluation': 0.6795796127098674} step=75\n",
      "Current state: [0.05328727], action: [0.], objective: 0.7248322147651006, reward: 0.7248322147651006\n",
      "Current state: [0.05328727], action: [-0.01493717], objective: 0.6993006993006993, reward: 0.6993006993006993\n",
      "Current state: [0.03835011], action: [0.], objective: 0.684931506849315, reward: 0.684931506849315\n",
      "Current state: [0.03835011], action: [0.], objective: 0.7083333333333334, reward: 0.7083333333333334\n",
      "Current state: [0.03835011], action: [-0.01446539], objective: 0.7450980392156863, reward: 0.7450980392156863\n",
      "Current state: [0.02388471], action: [0.006902], objective: 0.7152317880794702, reward: 0.7152317880794702\n",
      "Current state: [0.03078672], action: [0.], objective: 0.6887417218543046, reward: 0.6887417218543046\n",
      "Current state: [0.03078672], action: [0.], objective: 0.6530612244897959, reward: 0.6530612244897959\n",
      "Current state: [0.03078672], action: [0.0067126], objective: 0.7320261437908496, reward: 0.7320261437908496\n",
      "Current state: [0.03749932], action: [0.], objective: 0.6197183098591549, reward: 0.6197183098591549\n",
      "Current state: [0.03749932], action: [0.00653999], objective: 0.7532467532467533, reward: 0.7532467532467533\n",
      "Current state: [0.04403931], action: [0.], objective: 0.6944444444444444, reward: 0.6944444444444444\n",
      "Current state: [0.04403931], action: [0.00642038], objective: 0.7926829268292683, reward: 0.7926829268292683\n",
      "Current state: [0.05045969], action: [0.], objective: 0.7466666666666667, reward: 0.7466666666666667\n",
      "Current state: [0.05045969], action: [0.], objective: 0.72, reward: 0.72\n",
      "Current state: [0.05045969], action: [0.], objective: 0.7848101265822784, reward: 0.7848101265822784\n",
      "Current state: [0.05045969], action: [0.01046547], objective: 0.7435897435897436, reward: 0.7435897435897436\n",
      "Current state: [0.06092516], action: [0.], objective: 0.7333333333333333, reward: 0.7333333333333333\n",
      "Current state: [0.06092516], action: [0.], objective: 0.8125, reward: 0.8125\n",
      "Current state: [0.06092516], action: [0.], objective: 0.7375886524822695, reward: 0.7375886524822695\n",
      "Current state: [0.06092516], action: [0.], objective: 0.6521739130434783, reward: 0.6521739130434783\n",
      "Current state: [0.06092516], action: [0.01039628], objective: 0.7210884353741497, reward: 0.7210884353741497\n",
      "Current state: [0.07132145], action: [0.01036111], objective: 0.7320261437908496, reward: 0.7320261437908496\n",
      "Current state: [0.08168256], action: [0.], objective: 0.7866666666666666, reward: 0.7866666666666666\n",
      "Current state: [0.08168256], action: [0.0102817], objective: 0.7516778523489933, reward: 0.7516778523489933\n",
      "Current state: [0.11169592], action: [0.01211171], objective: 0.7746478873239436, reward: 0.7746478873239436\n",
      "True\n",
      "Current state: [0.39114096], action: [-0.01404993], objective: 0.9473684210526315, reward: 0.9473684210526315\n",
      "True\n",
      "Current state: [0.51670085], action: [-0.02496329], objective: 0.9411764705882353, reward: 0.9411764705882353\n",
      "True\n",
      "Current state: [0.33246099], action: [-0.0077305], objective: 0.921875, reward: 0.921875\n",
      "True\n",
      "Current state: [0.74633212], action: [-0.04241966], objective: 0.5641025641025641, reward: 0.5641025641025641\n",
      "True\n",
      "Current state: [0.71540048], action: [-0.03991688], objective: 0.6739130434782609, reward: 0.6739130434782609\n",
      "True\n",
      "Current state: [0.53882465], action: [-0.02646458], objective: 0.9, reward: 0.9\n",
      "True\n",
      "Current state: [0.48331698], action: [-0.02303932], objective: 0.9661016949152542, reward: 0.9661016949152542\n",
      "True\n",
      "Current state: [0.95611729], action: [-0.05895146], objective: 0.07547169811320754, reward: 0.07547169811320754\n",
      "True\n",
      "Current state: [0.60705237], action: [-0.03078534], objective: 0.7628865979381443, reward: 0.7628865979381443\n",
      "True\n",
      "2023-03-17 17:56:38 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230317175637/model_100.pt\n",
      "2023-03-17 17:56:38 [info     ] SAC_online_20230317175637: epoch=4 step=100 epoch=4 metrics={'time_inference': 0.0010579299926757812, 'time_environment_step': 0.00030709266662597655, 'time_step': 0.0030959129333496095, 'time_sample_batch': 0.00010244051615397136, 'time_algorithm_update': 0.013879617055257162, 'temp_loss': 1.6749095916748047, 'temp': 0.9981976747512817, 'critic_loss': 1.3391677935918171, 'actor_loss': -1.0507201353708904, 'evaluation': 0.7527543377512241} step=100\n"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import TimeLimit\n",
    "from d3rlpy.algos import DQN, CQL, SAC, DDPG\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy, ConstantEpsilonGreedy\n",
    "\n",
    "config = {'real_threshold': 0.45}\n",
    "env = Continuous_states(config)\n",
    "\n",
    "# setup algorithm\n",
    "cql = CQL(batch_size=32,\n",
    "          scaler='min_max',\n",
    "          learning_rate=2.5e-7,\n",
    "          target_update_interval=100,\n",
    "          use_gpu=False)\n",
    "\n",
    "# setup replay buffer\n",
    "buffer = ReplayBuffer(maxlen=100000, env=env)\n",
    "\n",
    "# setup explorers - if you increase steps, increase the duration for exploration as well otherwise it is \n",
    "# simply adding more episodes without exploration which is not helpful unless it has explored well\n",
    "explorer = LinearDecayEpsilonGreedy(start_epsilon=1.0,\n",
    "                                    end_epsilon=0.1,\n",
    "                                    duration=200)\n",
    "\n",
    "# start training\n",
    "cql.fit_online(env,\n",
    "               buffer,\n",
    "               explorer=explorer, # you don't need this with probablistic policy algorithms\n",
    "               eval_env=env,\n",
    "               n_steps=100, # the number of total steps to train.\n",
    "               n_steps_per_epoch=25,\n",
    "               update_interval=10) # update parameters every 10 steps.\n",
    "\n",
    "# export online dataset as MDPDataset\n",
    "dataset_online = buffer.to_mdp_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c0984",
   "metadata": {},
   "source": [
    "### Print only the highest reward state and action pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "749fd1df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[(array([0.45711476], dtype=float32), array([0.], dtype=float32), 1.0)] 1.0\n",
      "\n",
      "\n",
      "[(array([0.4665606], dtype=float32), array([0.], dtype=float32), 0.9904762)] 0.9904762\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[(array([0.52942276], dtype=float32), array([0.], dtype=float32), 0.95454544)] 0.95454544\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[(array([0.34988758], dtype=float32), array([0.], dtype=float32), 0.90163934)] 0.90163934\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[(array([0.28140953], dtype=float32), array([0.], dtype=float32), 0.9076923)] 0.9076923\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[(array([0.42825067], dtype=float32), array([0.], dtype=float32), 0.9904762)] 0.9904762\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[(array([0.48660102], dtype=float32), array([0.], dtype=float32), 0.991453)] 0.991453\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[(array([0.49816376], dtype=float32), array([0.], dtype=float32), 0.9433962)] 0.9433962\n",
      "\n",
      "\n",
      "[(array([0.5565454], dtype=float32), array([0.], dtype=float32), 0.90909094)] 0.90909094\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[(array([0.4785633], dtype=float32), array([0.], dtype=float32), 0.991453)] 0.991453\n",
      "\n",
      "\n",
      "[(array([0.4772746], dtype=float32), array([0.], dtype=float32), 0.98039216)] 0.98039216\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[(array([0.74721676], dtype=float32), array([0.], dtype=float32), 0.6933333), (array([0.7710324], dtype=float32), array([0.], dtype=float32), 0.55263156), (array([0.7710324], dtype=float32), array([0.], dtype=float32), 0.62857145), (array([0.7710324], dtype=float32), array([-0.12214544], dtype=float32), 0.57894737), (array([0.648887], dtype=float32), array([0.], dtype=float32), 0.7160494), (array([0.648887], dtype=float32), array([0.], dtype=float32), 0.84444445), (array([0.648887], dtype=float32), array([-0.10833202], dtype=float32), 0.8041237), (array([0.54055494], dtype=float32), array([0.], dtype=float32), 0.9072165), (array([0.54055494], dtype=float32), array([-0.09843021], dtype=float32), 0.96666664), (array([0.44212475], dtype=float32), array([-0.08868174], dtype=float32), 1.0), (array([0.353443], dtype=float32), array([-0.05502847], dtype=float32), 0.9508197), (array([0.29841453], dtype=float32), array([0.], dtype=float32), 0.9264706), (array([0.29841453], dtype=float32), array([0.], dtype=float32), 0.88235295), (array([0.29841453], dtype=float32), array([0.], dtype=float32), 0.84444445), (array([0.29841453], dtype=float32), array([0.], dtype=float32), 0.87096775), (array([0.29841453], dtype=float32), array([0.], dtype=float32), 0.90076333), (array([0.29841453], dtype=float32), array([0.], dtype=float32), 0.8833333), (array([0.29841453], dtype=float32), array([0.], dtype=float32), 0.85925925), (array([0.29841453], dtype=float32), array([0.], dtype=float32), 0.86885244), (array([0.29841453], dtype=float32), array([0.], dtype=float32), 0.8358209), (array([0.29841453], dtype=float32), array([-0.03243594], dtype=float32), 0.8939394), (array([0.2659786], dtype=float32), array([0.], dtype=float32), 0.89361703), (array([0.2659786], dtype=float32), array([0.], dtype=float32), 0.8740741), (array([0.2659786], dtype=float32), array([-0.02910129], dtype=float32), 0.896), (array([0.2368773], dtype=float32), array([0.], dtype=float32), 0.85714287), (array([0.2368773], dtype=float32), array([0.], dtype=float32), 0.72483224), (array([0.05328727], dtype=float32), array([-0.01493717], dtype=float32), 0.6993007), (array([0.03835011], dtype=float32), array([0.], dtype=float32), 0.6849315), (array([0.03835011], dtype=float32), array([0.], dtype=float32), 0.7083333), (array([0.03835011], dtype=float32), array([-0.01446539], dtype=float32), 0.74509805), (array([0.02388471], dtype=float32), array([0.006902], dtype=float32), 0.7152318), (array([0.03078672], dtype=float32), array([0.], dtype=float32), 0.68874174), (array([0.03078672], dtype=float32), array([0.], dtype=float32), 0.6530612), (array([0.03078672], dtype=float32), array([0.0067126], dtype=float32), 0.73202616), (array([0.03749932], dtype=float32), array([0.], dtype=float32), 0.6197183), (array([0.03749932], dtype=float32), array([0.00653999], dtype=float32), 0.7532467), (array([0.04403931], dtype=float32), array([0.], dtype=float32), 0.6944444), (array([0.04403931], dtype=float32), array([0.00642038], dtype=float32), 0.79268295), (array([0.05045969], dtype=float32), array([0.], dtype=float32), 0.74666667), (array([0.05045969], dtype=float32), array([0.], dtype=float32), 0.72), (array([0.05045969], dtype=float32), array([0.], dtype=float32), 0.7848101), (array([0.05045969], dtype=float32), array([0.01046547], dtype=float32), 0.74358976), (array([0.06092516], dtype=float32), array([0.], dtype=float32), 0.73333335), (array([0.06092516], dtype=float32), array([0.], dtype=float32), 0.8125), (array([0.06092516], dtype=float32), array([0.], dtype=float32), 0.73758864), (array([0.06092516], dtype=float32), array([0.], dtype=float32), 0.65217394), (array([0.06092516], dtype=float32), array([0.01039628], dtype=float32), 0.7210884), (array([0.07132145], dtype=float32), array([0.01036111], dtype=float32), 0.73202616), (array([0.08168256], dtype=float32), array([0.], dtype=float32), 0.7866667)] 38.311935\n"
     ]
    }
   ],
   "source": [
    "def get_episode_actions(dataset):\n",
    "    for episode in dataset.episodes:\n",
    "        print(episode.actions, episode.compute_return())\n",
    "        \n",
    "for episode in dataset_online.episodes:\n",
    "    print(\"\\n\")\n",
    "    episode_return = episode.compute_return()\n",
    "    if(episode_return > 0.9):\n",
    "        print(list(zip(episode.observations, episode.actions, episode.rewards)), episode_return)\n",
    "#get_episode_actions(dataset_online)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57821594",
   "metadata": {},
   "source": [
    "### Test on new environment with different threshold parameters but with the same RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "f800760d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-17 18:05:23 [info     ] Directory is created at d3rlpy_logs/SAC_online_20230317180523\n",
      "2023-03-17 18:05:23 [debug    ] Fitting scaler...              scler=min_max\n",
      "2023-03-17 18:05:23 [warning  ] Skip building models since they're already built.\n",
      "2023-03-17 18:05:23 [info     ] Parameters are saved to d3rlpy_logs/SAC_online_20230317180523/params.json params={'action_scaler': None, 'actor_encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'actor_learning_rate': 0.0003, 'actor_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'batch_size': 32, 'critic_encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'critic_learning_rate': 0.0003, 'critic_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'gamma': 0.99, 'generated_maxlen': 100000, 'initial_temperature': 1.0, 'n_critics': 2, 'n_frames': 1, 'n_steps': 1, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': {'type': 'min_max', 'params': {'maximum': array([[1.]], dtype=float32), 'minimum': array([[0.]], dtype=float32)}}, 'tau': 0.005, 'temp_learning_rate': 0.0003, 'temp_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'use_gpu': None, 'algorithm': 'SAC', 'observation_shape': (1,), 'action_size': 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa66ec8a109413ebe5283f3d99b1256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: [0.22309883], action: [0.], objective: 0.3838383838383838, reward: 0.3838383838383838\n",
      "True\n",
      "Current state: [0.1988061], action: [0.], objective: 0.44680851063829785, reward: 0.44680851063829785\n",
      "True\n",
      "Current state: [0.49959992], action: [0.], objective: 0.6376811594202898, reward: 0.6376811594202898\n",
      "True\n",
      "Current state: [0.08641219], action: [0.], objective: 0.4107142857142857, reward: 0.4107142857142857\n",
      "True\n",
      "Current state: [0.88786594], action: [0.], objective: 0.5454545454545454, reward: 0.5454545454545454\n",
      "True\n",
      "Current state: [0.31530217], action: [0.], objective: 0.5714285714285714, reward: 0.5714285714285714\n",
      "True\n",
      "Current state: [0.82251001], action: [0.], objective: 0.8717948717948718, reward: 0.8717948717948718\n",
      "True\n",
      "Current state: [0.744394], action: [0.], objective: 0.975609756097561, reward: 0.975609756097561\n",
      "True\n",
      "Current state: [0.57131803], action: [0.], objective: 0.75, reward: 0.75\n",
      "True\n",
      "Current state: [0.59213506], action: [0.], objective: 0.72, reward: 0.72\n",
      "True\n",
      "Current state: [0.95492402], action: [-0.0579323], objective: 0.2857142857142857, reward: 0.2857142857142857\n",
      "True\n",
      "Current state: [0.89052527], action: [0.], objective: 0.5, reward: 0.5\n",
      "True\n",
      "Current state: [0.30420425], action: [0.], objective: 0.6105263157894737, reward: 0.6105263157894737\n",
      "True\n",
      "Current state: [0.33668258], action: [0.], objective: 0.5656565656565656, reward: 0.5656565656565656\n",
      "True\n",
      "Current state: [0.18933302], action: [0.], objective: 0.48598130841121495, reward: 0.48598130841121495\n",
      "True\n",
      "Current state: [0.22507932], action: [0.], objective: 0.45652173913043476, reward: 0.45652173913043476\n",
      "True\n",
      "Current state: [0.63136767], action: [0.], objective: 0.7878787878787878, reward: 0.7878787878787878\n",
      "True\n",
      "Current state: [0.89519347], action: [0.], objective: 0.5384615384615384, reward: 0.5384615384615384\n",
      "True\n",
      "Current state: [0.84534129], action: [0.], objective: 0.7692307692307693, reward: 0.7692307692307693\n",
      "True\n",
      "Current state: [0.61332417], action: [0.], objective: 0.8253968253968254, reward: 0.8253968253968254\n",
      "True\n",
      "Current state: [0.28363341], action: [0.], objective: 0.4489795918367347, reward: 0.4489795918367347\n",
      "True\n",
      "Current state: [0.78220446], action: [0.], objective: 0.9411764705882353, reward: 0.9411764705882353\n",
      "True\n",
      "Current state: [0.05279745], action: [0.], objective: 0.4369747899159664, reward: 0.4369747899159664\n",
      "True\n",
      "Current state: [0.06948435], action: [0.], objective: 0.35398230088495575, reward: 0.35398230088495575\n",
      "True\n",
      "Current state: [0.585772], action: [-0.01810547], objective: 0.7857142857142857, reward: 0.7857142857142857\n",
      "True\n",
      "Current state: [0.42074443], action: [-0.00451737], objective: 0.5168539325842697, reward: 0.5168539325842697\n",
      "True\n",
      "Current state: [0.54171254], action: [-0.01512831], objective: 0.6666666666666666, reward: 0.6666666666666666\n",
      "True\n",
      "Current state: [0.81591908], action: [-0.03972427], objective: 0.9411764705882353, reward: 0.9411764705882353\n",
      "True\n",
      "Current state: [0.77963955], action: [-0.03645008], objective: 0.9333333333333333, reward: 0.9333333333333333\n",
      "True\n",
      "Current state: [0.8198764], action: [-0.04002993], objective: 0.8484848484848485, reward: 0.8484848484848485\n",
      "True\n",
      "Current state: [0.45886526], action: [-0.00872066], objective: 0.7012987012987013, reward: 0.7012987012987013\n",
      "True\n",
      "Current state: [0.90542052], action: [-0.046415], objective: 0.45454545454545453, reward: 0.45454545454545453\n",
      "True\n",
      "Current state: [0.92549134], action: [-0.04764332], objective: 0.5142857142857142, reward: 0.5142857142857142\n",
      "True\n",
      "Current state: [0.42682565], action: [-0.00524734], objective: 0.5405405405405406, reward: 0.5405405405405406\n",
      "True\n",
      "Current state: [0.33570678], action: [0.00524345], objective: 0.5858585858585859, reward: 0.5858585858585859\n",
      "True\n",
      "2023-03-17 18:05:23 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230317180523/model_25.pt\n",
      "2023-03-17 18:05:23 [info     ] SAC_online_20230317180523: epoch=1 step=25 epoch=1 metrics={'time_inference': 0.0011029052734375, 'time_environment_step': 0.0003220558166503906, 'rollout_return': 0.6042210263598753, 'time_step': 0.0028800296783447264, 'time_sample_batch': 0.00011086463928222656, 'time_algorithm_update': 0.01657271385192871, 'temp_loss': 1.659774124622345, 'temp': 0.9974474608898163, 'critic_loss': 0.9159444272518158, 'actor_loss': -1.1439316272735596, 'evaluation': 0.670304424818635} step=25\n",
      "Current state: [0.34095024], action: [0.], objective: 0.58, reward: 0.58\n",
      "Current state: [0.34095024], action: [0.], objective: 0.6796116504854369, reward: 0.6796116504854369\n",
      "Current state: [0.34095024], action: [0.00467383], objective: 0.4883720930232558, reward: 0.4883720930232558\n",
      "Current state: [0.34562406], action: [0.], objective: 0.5773195876288659, reward: 0.5773195876288659\n",
      "Current state: [0.34562406], action: [0.], objective: 0.5531914893617021, reward: 0.5531914893617021\n",
      "Current state: [0.34562406], action: [0.], objective: 0.6021505376344086, reward: 0.6021505376344086\n",
      "Current state: [0.34562406], action: [0.], objective: 0.5742574257425742, reward: 0.5742574257425742\n",
      "Current state: [0.34562406], action: [0.], objective: 0.5, reward: 0.5\n",
      "Current state: [0.34562406], action: [0.02260846], objective: 0.5714285714285714, reward: 0.5714285714285714\n",
      "Current state: [0.36823252], action: [0.02034042], objective: 0.6213592233009708, reward: 0.6213592233009708\n",
      "Current state: [0.38857294], action: [0.], objective: 0.5168539325842697, reward: 0.5168539325842697\n",
      "Current state: [0.38857294], action: [0.], objective: 0.62, reward: 0.62\n",
      "Current state: [0.38857294], action: [0.], objective: 0.4931506849315068, reward: 0.4931506849315068\n",
      "Current state: [0.38857294], action: [0.01827924], objective: 0.7111111111111111, reward: 0.7111111111111111\n",
      "Current state: [0.40685218], action: [0.], objective: 0.6153846153846154, reward: 0.6153846153846154\n",
      "Current state: [0.40685218], action: [0.03556712], objective: 0.5747126436781609, reward: 0.5747126436781609\n",
      "Current state: [0.4424193], action: [0.], objective: 0.5384615384615384, reward: 0.5384615384615384\n",
      "Current state: [0.4424193], action: [0.], objective: 0.64, reward: 0.64\n",
      "Current state: [0.4424193], action: [0.], objective: 0.5, reward: 0.5\n",
      "Current state: [0.4424193], action: [0.], objective: 0.7341772151898734, reward: 0.7341772151898734\n",
      "Current state: [0.4424193], action: [0.], objective: 0.5925925925925926, reward: 0.5925925925925926\n",
      "Current state: [0.4424193], action: [0.], objective: 0.6756756756756757, reward: 0.6756756756756757\n",
      "Current state: [0.4424193], action: [0.03187467], objective: 0.6835443037974683, reward: 0.6835443037974683\n",
      "Current state: [0.47429396], action: [0.029549], objective: 0.6741573033707865, reward: 0.6741573033707865\n",
      "Current state: [0.50384296], action: [0.], objective: 0.6585365853658537, reward: 0.6585365853658537\n",
      "Current state: [0.95415612], action: [0.02451471], objective: 0.06666666666666667, reward: 0.06666666666666667\n",
      "True\n",
      "Current state: [0.72060311], action: [0.03539888], objective: 0.90625, reward: 0.90625\n",
      "True\n",
      "Current state: [0.03339822], action: [0.08061893], objective: 0.4098360655737705, reward: 0.4098360655737705\n",
      "True\n",
      "Current state: [0.77196536], action: [0.03250686], objective: 0.972972972972973, reward: 0.972972972972973\n",
      "True\n",
      "Current state: [0.47202131], action: [0.04969307], objective: 0.6470588235294118, reward: 0.6470588235294118\n",
      "True\n",
      "Current state: [0.90003939], action: [0.02602386], objective: 0.6666666666666666, reward: 0.6666666666666666\n",
      "True\n",
      "Current state: [0.86421338], action: [0.02745765], objective: 0.7450980392156863, reward: 0.7450980392156863\n",
      "True\n",
      "Current state: [0.66588123], action: [0.03914231], objective: 0.8771929824561403, reward: 0.8771929824561403\n",
      "True\n",
      "Current state: [0.71346528], action: [0.0358323], objective: 0.9310344827586207, reward: 0.9310344827586207\n",
      "True\n",
      "Current state: [0.13962422], action: [0.07877238], objective: 0.4528301886792453, reward: 0.4528301886792453\n",
      "True\n",
      "2023-03-17 18:05:23 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230317180523/model_50.pt\n",
      "2023-03-17 18:05:23 [info     ] SAC_online_20230317180523: epoch=2 step=50 epoch=2 metrics={'time_inference': 0.0013391685485839844, 'time_environment_step': 0.0003778076171875, 'time_step': 0.0036963844299316407, 'time_sample_batch': 0.00011444091796875, 'time_algorithm_update': 0.01586771011352539, 'temp_loss': 1.651676098505656, 'temp': 0.9966994325319926, 'critic_loss': 0.7610830068588257, 'actor_loss': -1.3232651154200237, 'evaluation': 0.6675606888519181} step=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: [0.2183966], action: [0.04793759], objective: 0.40425531914893614, reward: 0.40425531914893614\n",
      "Current state: [0.26633419], action: [0.], objective: 0.43010752688172044, reward: 0.43010752688172044\n",
      "Current state: [0.26633419], action: [0.], objective: 0.4842105263157895, reward: 0.4842105263157895\n",
      "Current state: [0.26633419], action: [0.], objective: 0.5531914893617021, reward: 0.5531914893617021\n",
      "Current state: [0.26633419], action: [0.06762098], objective: 0.5048543689320388, reward: 0.5048543689320388\n",
      "Current state: [0.33395516], action: [0.], objective: 0.5050505050505051, reward: 0.5050505050505051\n",
      "Current state: [0.33395516], action: [0.06131301], objective: 0.5161290322580645, reward: 0.5161290322580645\n",
      "Current state: [0.39526818], action: [0.], objective: 0.6428571428571429, reward: 0.6428571428571429\n",
      "Current state: [0.39526818], action: [0.], objective: 0.5617977528089888, reward: 0.5617977528089888\n",
      "Current state: [0.39526818], action: [0.], objective: 0.6896551724137931, reward: 0.6896551724137931\n",
      "Current state: [0.39526818], action: [0.06221671], objective: 0.525, reward: 0.525\n",
      "Current state: [0.45748488], action: [0.], objective: 0.5977011494252874, reward: 0.5977011494252874\n",
      "Current state: [0.45748488], action: [0.05683564], objective: 0.5517241379310345, reward: 0.5517241379310345\n",
      "Current state: [0.51432052], action: [0.05337678], objective: 0.64, reward: 0.64\n",
      "Current state: [0.56769729], action: [0.], objective: 0.8253968253968254, reward: 0.8253968253968254\n",
      "Current state: [0.56769729], action: [0.], objective: 0.6229508196721312, reward: 0.6229508196721312\n",
      "Current state: [0.56769729], action: [0.05056409], objective: 0.8, reward: 0.8\n",
      "Current state: [0.61826139], action: [0.], objective: 0.875, reward: 0.875\n",
      "Current state: [0.61826139], action: [0.0479298], objective: 0.8421052631578947, reward: 0.8421052631578947\n",
      "Current state: [0.66619119], action: [0.], objective: 0.875, reward: 0.875\n",
      "Current state: [0.66619119], action: [0.], objective: 0.8333333333333334, reward: 0.8333333333333334\n",
      "Current state: [0.66619119], action: [0.04620238], objective: 0.8955223880597015, reward: 0.8955223880597015\n",
      "Current state: [0.71239356], action: [0.04302885], objective: 0.9180327868852459, reward: 0.9180327868852459\n",
      "Current state: [0.75542242], action: [0.], objective: 0.9836065573770492, reward: 0.9836065573770492\n",
      "Current state: [0.75542242], action: [0.04094364], objective: 1.0, reward: 1.0\n",
      "Current state: [0.29370235], action: [0.07306763], objective: 0.5283018867924528, reward: 0.5283018867924528\n",
      "True\n",
      "Current state: [0.4646468], action: [0.05706139], objective: 0.6976744186046512, reward: 0.6976744186046512\n",
      "True\n",
      "Current state: [0.95080793], action: [0.0329592], objective: 0.42424242424242425, reward: 0.42424242424242425\n",
      "True\n",
      "Current state: [0.93196273], action: [0.03341437], objective: 0.631578947368421, reward: 0.631578947368421\n",
      "True\n",
      "Current state: [0.06503779], action: [0.088327], objective: 0.42735042735042733, reward: 0.42735042735042733\n",
      "True\n",
      "Current state: [0.98816945], action: [0.03257924], objective: 0.08, reward: 0.08\n",
      "True\n",
      "Current state: [0.85159077], action: [0.03624564], objective: 0.7142857142857143, reward: 0.7142857142857143\n",
      "True\n",
      "Current state: [0.78328767], action: [0.03926585], objective: 0.9333333333333333, reward: 0.9333333333333333\n",
      "True\n",
      "Current state: [0.34606518], action: [0.06820904], objective: 0.5567010309278351, reward: 0.5567010309278351\n",
      "True\n",
      "Current state: [0.33048285], action: [0.06963226], objective: 0.41975308641975306, reward: 0.41975308641975306\n",
      "True\n",
      "2023-03-17 18:05:23 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230317180523/model_75.pt\n",
      "2023-03-17 18:05:23 [info     ] SAC_online_20230317180523: epoch=3 step=75 epoch=3 metrics={'time_inference': 0.0011440181732177734, 'time_environment_step': 0.00035821914672851563, 'time_step': 0.0026270675659179686, 'time_sample_batch': 9.560585021972656e-05, 'time_algorithm_update': 0.013222336769104004, 'temp_loss': 1.6707883477210999, 'temp': 0.9959514737129211, 'critic_loss': 0.5365105122327805, 'actor_loss': -1.4628790020942688, 'evaluation': 0.5413221269325013} step=75\n",
      "Current state: [0.40011511], action: [0.], objective: 0.6236559139784946, reward: 0.6236559139784946\n",
      "Current state: [0.40011511], action: [0.], objective: 0.5319148936170213, reward: 0.5319148936170213\n",
      "Current state: [0.40011511], action: [0.06290127], objective: 0.6458333333333334, reward: 0.6458333333333334\n",
      "Current state: [0.46301638], action: [0.05717839], objective: 0.5753424657534246, reward: 0.5753424657534246\n",
      "Current state: [0.52019477], action: [0.], objective: 0.7714285714285715, reward: 0.7714285714285715\n",
      "Current state: [0.52019477], action: [0.], objective: 0.7777777777777778, reward: 0.7777777777777778\n",
      "Current state: [0.52019477], action: [0.], objective: 0.6666666666666666, reward: 0.6666666666666666\n",
      "Current state: [0.52019477], action: [0.06563334], objective: 0.7222222222222222, reward: 0.7222222222222222\n",
      "Current state: [0.58582812], action: [0.], objective: 0.7666666666666667, reward: 0.7666666666666667\n",
      "Current state: [0.58582812], action: [0.], objective: 0.6666666666666666, reward: 0.6666666666666666\n",
      "Current state: [0.58582812], action: [0.], objective: 0.6557377049180327, reward: 0.6557377049180327\n",
      "Current state: [0.58582812], action: [0.], objective: 0.7575757575757576, reward: 0.7575757575757576\n",
      "Current state: [0.58582812], action: [0.06240358], objective: 0.7368421052631579, reward: 0.7368421052631579\n",
      "Current state: [0.64823169], action: [0.], objective: 0.8679245283018868, reward: 0.8679245283018868\n",
      "Current state: [0.64823169], action: [0.], objective: 0.6037735849056604, reward: 0.6037735849056604\n",
      "Current state: [0.64823169], action: [0.], objective: 0.8709677419354839, reward: 0.8709677419354839\n",
      "Current state: [0.64823169], action: [0.], objective: 0.9152542372881356, reward: 0.9152542372881356\n",
      "Current state: [0.64823169], action: [0.08013088], objective: 0.7142857142857143, reward: 0.7142857142857143\n",
      "Current state: [0.72836258], action: [0.], objective: 1.0, reward: 1.0\n",
      "Current state: [0.72836258], action: [0.07607986], objective: 0.9361702127659575, reward: 0.9361702127659575\n",
      "Current state: [0.80444244], action: [0.07335438], objective: 0.8928571428571429, reward: 0.8928571428571429\n",
      "Current state: [0.87779681], action: [0.], objective: 0.7555555555555555, reward: 0.7555555555555555\n",
      "Current state: [0.87779681], action: [0.], objective: 0.5714285714285714, reward: 0.5714285714285714\n",
      "Current state: [0.87779681], action: [0.], objective: 0.5405405405405406, reward: 0.5405405405405406\n",
      "Current state: [0.87779681], action: [0.07161643], objective: 0.6530612244897959, reward: 0.6530612244897959\n",
      "Current state: [0.05766242], action: [0.11831429], objective: 0.43548387096774194, reward: 0.43548387096774194\n",
      "True\n",
      "Current state: [0.95820049], action: [0.08832417], objective: 0.1, reward: 0.1\n",
      "True\n",
      "Current state: [0.8294117], action: [0.09030075], objective: 0.8235294117647058, reward: 0.8235294117647058\n",
      "True\n",
      "Current state: [0.14327976], action: [0.11917073], objective: 0.40350877192982454, reward: 0.40350877192982454\n",
      "True\n",
      "Current state: [0.738568], action: [0.09209732], objective: 1.0, reward: 1.0\n",
      "True\n",
      "Current state: [0.11525405], action: [0.11939566], objective: 0.47368421052631576, reward: 0.47368421052631576\n",
      "True\n",
      "Current state: [0.45732661], action: [0.10076634], objective: 0.6436781609195402, reward: 0.6436781609195402\n",
      "True\n",
      "Current state: [0.35395856], action: [0.10665399], objective: 0.449438202247191, reward: 0.449438202247191\n",
      "True\n",
      "Current state: [0.52284938], action: [0.09833892], objective: 0.6666666666666666, reward: 0.6666666666666666\n",
      "True\n",
      "Current state: [0.71864102], action: [0.09260388], objective: 0.9411764705882353, reward: 0.9411764705882353\n",
      "True\n",
      "2023-03-17 18:05:23 [info     ] Model parameters are saved to d3rlpy_logs/SAC_online_20230317180523/model_100.pt\n",
      "2023-03-17 18:05:23 [info     ] SAC_online_20230317180523: epoch=4 step=100 epoch=4 metrics={'time_inference': 0.0010728645324707031, 'time_environment_step': 0.0003067779541015625, 'time_step': 0.0030230712890625, 'time_sample_batch': 0.00010085105895996094, 'time_algorithm_update': 0.013155380884806315, 'temp_loss': 1.6455308596293132, 'temp': 0.9952053229014078, 'critic_loss': 0.4437343279520671, 'actor_loss': -1.6294519503911336, 'evaluation': 0.5937165765610222} step=100\n"
     ]
    }
   ],
   "source": [
    "config = {'real_threshold': 0.75}\n",
    "env = Continuous_states(config)\n",
    "\n",
    "# start training\n",
    "cql.fit_online(env,\n",
    "               buffer,\n",
    "               explorer=explorer, # you don't need this with probablistic policy algorithms\n",
    "               eval_env=env,\n",
    "               n_steps=100, # the number of total steps to train.\n",
    "               n_steps_per_epoch=25,\n",
    "               update_interval=10) # update parameters every 10 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb94505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfa6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
